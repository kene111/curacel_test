{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-balanced-batch-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "#from keras.utils.data_utils import Sequence\n",
    "from keras_balanced_batch_generator import make_generator\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.keras import balanced_batch_generator\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_data =  \"./car_image_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images into memory\n",
    "def load_dataset(img_folder):\n",
    "    photos, targets = list(), list()\n",
    "    # enumerate files in the directory\n",
    "    print('Preparing Data ...')\n",
    "    print(' ')\n",
    "    for filename in listdir(img_folder):\n",
    "        # load image\n",
    "        if filename != 'Thumbs.db':\n",
    "           # print(filename)\n",
    "            photo = load_img(img_folder + filename, target_size =(224,224))\n",
    "            # convert to numpy array\n",
    "            photo = img_to_array(photo, dtype='uint8')\n",
    "            \n",
    "            # get targets.\n",
    "            target = filename.split('-')[0]\n",
    "            \n",
    "            # store\n",
    "            photos.append(photo)\n",
    "            targets.append(target)\n",
    "    print('Done!')\n",
    "            \n",
    "    X = asarray(photos, dtype = 'uint8')\n",
    "    Y = asarray(targets) #, dtype = 'uint8'\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_dataset(car_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both arrays  and targets to one file in compressed format\n",
    "savez_compressed('compressed_data.npz', X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from compressed data\n",
    "data = load('/kaggle/input/toyotacars/compressed_data.npz')    \n",
    "x,y = data['arr_0'], data['arr_1']\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder\n",
    "ohe = preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating the dataset into train and test datasets\n",
    "\n",
    "def split_dataset(X, Y):\n",
    "    train_x, test_x,train_y,test_y = train_test_split(X, Y, test_size = 0.1, random_state =3)\n",
    "    return(train_x, test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_model(input_shape =(224,224,3), output_shape = None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer ='he_uniform'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer ='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    # compile model\n",
    "    #opt = SGD(lr=0.03, momentum =0.9)\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer = opt, loss ='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the learning curves, this function will show the loss and the accuracy\n",
    "\n",
    "def summary(info):\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot (info.history['loss'], color='blue', label='train')\n",
    "    plt.plot (info.history['val_loss'], color='orange', label='test')\n",
    "    plt.legend([\"Loss\",\"Validation Loss\"])\n",
    "    \n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot (info.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot (info.history['val_accuracy'], color='orange', label='test')\n",
    "    plt.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_sequence(X,Y):\n",
    "    # split dataset\n",
    "    train_x, test_x,train_y,test_y = split_dataset(X, Y)\n",
    "    \n",
    "    \n",
    "    #create data generator\n",
    "    train_datagen = ImageDataGenerator(rescale =1.0/255.0,  horizontal_flip = True, vertical_flip = True, rotation_range = 90)\n",
    "    test_datagen = ImageDataGenerator(rescale =1.0/255.0)\n",
    "    \n",
    "    \n",
    "    ohe.fit(train_y.reshape(-1,1))\n",
    "    y_encoded_train = ohe.transform(train_y.reshape(-1,1)).toarray()\n",
    "    y_encoded_test = ohe.transform(test_y.reshape(-1,1)).toarray()\n",
    "    \n",
    "    \n",
    "    # prepare iterations\n",
    "    train = train_datagen.flow(train_x, y_encoded_train, batch_size = 32)\n",
    "    test = test_datagen.flow(test_x, y_encoded_test, batch_size = 32)\n",
    "    \n",
    "    # define the model\n",
    "    model = image_model(output_shape = y_encoded_train.shape[1])\n",
    "   \n",
    "    # handles the unbalanced image dataset\n",
    "    generator = make_generator(train_x,y_encoded_train, batch_size=32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    info = model.fit_generator(generator, steps_per_epoch = len(train), validation_data = test, \n",
    "                                 validation_steps = len(test), epochs =50, verbose = 1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate_generator(test, steps = len(test), verbose = 1)\n",
    "    print('> loss = %.3f, accuracy=%.3f' %(loss, accuracy))\n",
    "                               \n",
    "    # saving the model\n",
    "    model.save('toyota_model_1.h5')\n",
    "                               \n",
    "    # display learning curves\n",
    "    summary(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learning_sequence(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x,train_y,test_y = split_dataset(x, y)\n",
    "ohe.fit(train_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_images = './multi_image_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = './single_image_test/rav4-2129563200.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_sequence(ohe, dirc=None,single_image=None,condition=False):\n",
    "    # load model\n",
    "    model = load_model('./toyota_model_1.h5')\n",
    "    \n",
    "    result = list()\n",
    "    \n",
    "    if condition:\n",
    "        for files in listdir(dirc):\n",
    "            if files != 'Thumbs.db':\n",
    "                photo = load_img(dirc + files, target_size =(224,224))\n",
    "                photo = img_to_array(photo, dtype='uint8')\n",
    "                photo = photo.reshape(1, 224, 224, 3)\n",
    "                prediction = model.predict(photo)\n",
    "                result.append(ohe.inverse_transform(prediction))\n",
    "                \n",
    "    else:\n",
    "        photo = load_img(single_image, target_size =(224,224))\n",
    "        photo = img_to_array(photo, dtype='uint8')\n",
    "        photo = photo.reshape(1, 224, 224, 3)\n",
    "        prediction = model.predict(photo)\n",
    "        result.append(ohe.inverse_transform(prediction))\n",
    "        \n",
    "        \n",
    "    return(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple images Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prediction_sequence(ohe, dirc=multiple_images,single_image=None,condition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prediction_sequence(ohe, dirc=None,single_image=single_image,condition=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/keras-balanced-batch-generator/\n",
    "#https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9\n",
    "#https://www.kaggle.com/occultainsights/toyota-cars-over-20k-labeled-images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
